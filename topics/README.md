1. Run `python get_consultation_data.py`
    - This will generate the CSV `consultations.csv` which has structure `| consultation_id | organisation | time | time | url | summary | description | raw_text |`
    - The `raw_text` field is generated by concatenating the raw text extracted from all linked PDF documents on the consultation

2. Run `Rscript tag_consultations.R`
    - This imports `consultations.csv` as generated by `get_consultation_data.py` and uses a TermDocumentMatrix to define representative keywords for each consultation.
    - These keywords are then compressed to a comma-separated string and saved into a new file `consultation_tags.csv`
    - This requires `tm` package installed in R.

3. Run `python get_twitter_data.py`
    - This takes a Twitter user's handle (specified in the code) and grabs status text and raw text of links tweeted for last 100 tweets, outputting to file
    - Uses some libraries (e.g. tweepy) not specified yet in requirements.txt

4. Run `Rscript tag_twitter_data.R`
    - Very similar to other scripts, looks in `/twitterati` dir for data, creates a corpus, tags it out, saves to `twitter_tags.csv`

5. Run `Rscript match_tags.R`
    - Reads in the consultation_tags.csv and twitter_tags.csv tag documents, and compares intersections of each user's tags against each consultation's tags for a basic recommendation engine
    - Prints results to screen for now
